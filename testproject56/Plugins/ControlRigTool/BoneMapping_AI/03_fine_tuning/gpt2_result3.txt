C:\Users\Administrator\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
C:\Users\Administrator\AppData\Local\Programs\Python\Python311\Lib\site-packages\peft\tuners\lora\layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
============================================================
[Bone Mapping AI] Fine-tuning Start (PEFT Mode)
============================================================
[OK] GPU: NVIDIA GeForce RTX 4090
[OK] VRAM: 25.8 GB

[1/6] Loading model...
[OK] Model loaded: gpt2

[2/6] Adding LoRA adapters...
trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867
[OK] LoRA applied (r=16, alpha=32)

[3/6] Loading dataset...
[OK] Train: 161 examples
[OK] Validation: 18 examples
Map:   0%|          | 0/18 [00:00<?, ? examples/s]Map: 100%|##########| 18/18 [00:00<00:00, 634.99 examples/s]
[OK] Dataset formatted and tokenized

[4/6] Setting up trainer...
[OK] Trainer configured

[5/6] Starting training...
   Epochs: 3
   Batch size: 1 x 8 = 8
   Learning rate: 0.0002
------------------------------------------------------------
  0%|          | 0/60 [00:00<?, ?it/s]  2%|1         | 1/60 [00:00<00:27,  2.17it/s]  3%|3         | 2/60 [00:00<00:21,  2.67it/s]  5%|5         | 3/60 [00:01<00:19,  2.88it/s]  7%|6         | 4/60 [00:01<00:18,  3.02it/s]  8%|8         | 5/60 [00:01<00:17,  3.07it/s]                                                8%|8         | 5/60 [00:01<00:17,  3.07it/s] 10%|#         | 6/60 [00:02<00:17,  3.13it/s] 12%|#1        | 7/60 [00:02<00:16,  3.19it/s] 13%|#3        | 8/60 [00:02<00:15,  3.39it/s] 15%|#5        | 9/60 [00:02<00:14,  3.46it/s] 17%|#6        | 10/60 [00:03<00:13,  3.64it/s]                                                17%|#6        | 10/60 [00:03<00:13,  3.64it/s] 18%|#8        | 11/60 [00:03<00:13,  3.67it/s] 20%|##        | 12/60 [00:03<00:12,  3.72it/s] 22%|##1       | 13/60 [00:03<00:13,  3.59it/s] 23%|##3       | 14/60 [00:04<00:13,  3.48it/s] 25%|##5       | 15/60 [00:04<00:13,  3.29it/s]                                                25%|##5       | 15/60 [00:04<00:13,  3.29it/s] 27%|##6       | 16/60 [00:04<00:13,  3.26it/s] 28%|##8       | 17/60 [00:05<00:12,  3.36it/s] 30%|###       | 18/60 [00:05<00:12,  3.33it/s] 32%|###1      | 19/60 [00:05<00:13,  3.11it/s] 33%|###3      | 20/60 [00:06<00:13,  2.88it/s]                                                33%|###3      | 20/60 [00:06<00:13,  2.88it/s] 35%|###5      | 21/60 [00:06<00:13,  2.95it/s] 37%|###6      | 22/60 [00:06<00:12,  2.95it/s] 38%|###8      | 23/60 [00:07<00:11,  3.12it/s] 40%|####      | 24/60 [00:07<00:11,  3.09it/s] 42%|####1     | 25/60 [00:07<00:11,  3.12it/s]                                                42%|####1     | 25/60 [00:07<00:11,  3.12it/s]{'loss': 3.6933, 'grad_norm': 0.6778173446655273, 'learning_rate': 0.0001, 'epoch': 0.25}
{'loss': 3.3491, 'grad_norm': 0.6424193978309631, 'learning_rate': 0.0002, 'epoch': 0.5}
{'loss': 3.4316, 'grad_norm': 0.8783674240112305, 'learning_rate': 0.00019510565162951537, 'epoch': 0.75}
{'loss': 3.0587, 'grad_norm': 0.7798224687576294, 'learning_rate': 0.00018090169943749476, 'epoch': 0.99}
{'loss': 2.7813, 'grad_norm': 0.8915672898292542, 'learning_rate': 0.00015877852522924732, 'epoch': 1.24}

  0%|          | 0/18 [00:00<?, ?it/s][A
 33%|###3      | 6/18 [00:00<00:00, 50.93it/s][A
 67%|######6   | 12/18 [00:00<00:00, 45.98it/s][A
 94%|#########4| 17/18 [00:00<00:00, 42.16it/s][A                                               
                                               [A 42%|####1     | 25/60 [00:08<00:11,  3.12it/s]
100%|##########| 18/18 [00:00<00:00, 42.16it/s][A
                                               [A 43%|####3     | 26/60 [00:09<00:19,  1.70it/s] 45%|####5     | 27/60 [00:09<00:16,  2.06it/s] 47%|####6     | 28/60 [00:09<00:14,  2.26it/s] 48%|####8     | 29/60 [00:09<00:12,  2.55it/s] 50%|#####     | 30/60 [00:10<00:10,  2.86it/s]                                                50%|#####     | 30/60 [00:10<00:10,  2.86it/s] 52%|#####1    | 31/60 [00:10<00:10,  2.85it/s] 53%|#####3    | 32/60 [00:10<00:09,  3.01it/s] 55%|#####5    | 33/60 [00:11<00:08,  3.25it/s] 57%|#####6    | 34/60 [00:11<00:07,  3.41it/s] 58%|#####8    | 35/60 [00:11<00:07,  3.54it/s]                                                58%|#####8    | 35/60 [00:11<00:07,  3.54it/s] 60%|######    | 36/60 [00:11<00:06,  3.52it/s] 62%|######1   | 37/60 [00:12<00:06,  3.37it/s] 63%|######3   | 38/60 [00:12<00:06,  3.40it/s] 65%|######5   | 39/60 [00:12<00:06,  3.31it/s] 67%|######6   | 40/60 [00:13<00:06,  3.13it/s]                                                67%|######6   | 40/60 [00:13<00:06,  3.13it/s] 68%|######8   | 41/60 [00:13<00:06,  3.11it/s] 70%|#######   | 42/60 [00:13<00:05,  3.22it/s] 72%|#######1  | 43/60 [00:14<00:05,  3.40it/s] 73%|#######3  | 44/60 [00:14<00:04,  3.53it/s] 75%|#######5  | 45/60 [00:14<00:04,  3.25it/s]                                                75%|#######5  | 45/60 [00:14<00:04,  3.25it/s] 77%|#######6  | 46/60 [00:14<00:04,  3.22it/s] 78%|#######8  | 47/60 [00:15<00:04,  3.14it/s] 80%|########  | 48/60 [00:15<00:03,  3.20it/s] 82%|########1 | 49/60 [00:15<00:03,  3.10it/s] 83%|########3 | 50/60 [00:16<00:03,  2.99it/s]                                                83%|########3 | 50/60 [00:16<00:03,  2.99it/s]{'eval_loss': 2.495199680328369, 'eval_runtime': 0.4386, 'eval_samples_per_second': 41.036, 'eval_steps_per_second': 41.036, 'epoch': 1.24}
{'loss': 2.2391, 'grad_norm': 0.99012690782547, 'learning_rate': 0.00013090169943749476, 'epoch': 1.49}
{'loss': 2.2816, 'grad_norm': 1.0290484428405762, 'learning_rate': 0.0001, 'epoch': 1.74}
{'loss': 2.018, 'grad_norm': 1.0627515316009521, 'learning_rate': 6.909830056250527e-05, 'epoch': 1.99}
{'loss': 1.8235, 'grad_norm': 0.902975857257843, 'learning_rate': 4.12214747707527e-05, 'epoch': 2.24}
{'loss': 1.8961, 'grad_norm': 1.0538287162780762, 'learning_rate': 1.9098300562505266e-05, 'epoch': 2.48}

  0%|          | 0/18 [00:00<?, ?it/s][A
 44%|####4     | 8/18 [00:00<00:00, 71.59it/s][A
 89%|########8 | 16/18 [00:00<00:00, 74.78it/s][A                                               
                                               [A 83%|########3 | 50/60 [00:16<00:03,  2.99it/s]
100%|##########| 18/18 [00:00<00:00, 74.78it/s][A
                                               [A 85%|########5 | 51/60 [00:17<00:04,  1.85it/s] 87%|########6 | 52/60 [00:17<00:03,  2.12it/s] 88%|########8 | 53/60 [00:17<00:02,  2.38it/s] 90%|######### | 54/60 [00:18<00:02,  2.49it/s] 92%|#########1| 55/60 [00:18<00:01,  2.59it/s]                                                92%|#########1| 55/60 [00:18<00:01,  2.59it/s] 93%|#########3| 56/60 [00:18<00:01,  2.74it/s] 95%|#########5| 57/60 [00:19<00:00,  3.02it/s] 97%|#########6| 58/60 [00:19<00:00,  3.08it/s] 98%|#########8| 59/60 [00:19<00:00,  3.19it/s]100%|##########| 60/60 [00:20<00:00,  3.32it/s]                                               100%|##########| 60/60 [00:20<00:00,  3.32it/s]                                               100%|##########| 60/60 [00:20<00:00,  3.32it/s]100%|##########| 60/60 [00:20<00:00,  2.91it/s]
{'eval_loss': 1.6574259996414185, 'eval_runtime': 0.2619, 'eval_samples_per_second': 68.718, 'eval_steps_per_second': 68.718, 'epoch': 2.48}
{'loss': 1.7427, 'grad_norm': 1.0017096996307373, 'learning_rate': 4.8943483704846475e-06, 'epoch': 2.73}
{'loss': 1.8161, 'grad_norm': 0.8670098781585693, 'learning_rate': 0.0, 'epoch': 2.98}
{'train_runtime': 20.5945, 'train_samples_per_second': 23.453, 'train_steps_per_second': 2.913, 'train_loss': 2.5109317938486737, 'epoch': 2.98}
------------------------------------------------------------
[OK] Training completed in 0:00:20.732003

[6/6] Saving model...
[OK] LoRA adapter saved: ./checkpoints\bone_mapping_lora

============================================================
[SUCCESS] Fine-tuning Complete!
============================================================
[OK] Config saved: ./checkpoints\training_config.json
