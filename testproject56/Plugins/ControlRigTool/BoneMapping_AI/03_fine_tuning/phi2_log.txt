============================================================
[Bone Mapping AI] Fine-tuning Start (PEFT Mode)
============================================================
[OK] GPU: NVIDIA GeForce RTX 4090
[OK] VRAM: 25.8 GB

[1/6] Loading model...
[OK] Using 4-bit quantization
C:\Users\Administrator\AppData\Local\Programs\Python\Python311\python.exe : 
위치 줄:1 문자:133
+ ... "Continue"; C:\Users\Administrator\AppData\Local\Programs\Python\Pyth ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError
 
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Xet Storage is enabled
 for this repo, but the 'hf_xet' package is not installed. Falling back to regu
lar HTTP download. For better performance, install the package with: `pip insta
ll huggingface_hub[hf_xet]` or `pip install hf_xet`

Downloading shards:  50%|#####     | 1/2 [01:39<01:39, 99.05s/it]
Downloading shards: 100%|##########| 2/2 [01:39<00:00, 49.53s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|#####     | 1/2 [00:02<00:02,  2.53s/it]
Loading checkpoint shards: 100%|##########| 2/2 [00:02<00:00,  1.27s/it]
Loading checkpoint shards: 100%|##########| 2/2 [00:02<00:00,  1.46s/it]
Traceback (most recent call last):
  File "E:\AI\AI_ControlRig_02\BoneMapping_AI\03_fine_tuning\train.py", line 36
9, in <module>
    main()
  File "E:\AI\AI_ControlRig_02\BoneMapping_AI\03_fine_tuning\train.py", line 10
6, in main
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python311\Lib\site
-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrain
ed
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python311\Lib\site
-packages\transformers\modeling_utils.py", line 4015, in from_pretrained
    dispatch_model(model, **device_map_kwargs)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python311\Lib\site
-packages\accelerate\big_modeling.py", line 502, in dispatch_model
    model.to(device)
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python311\Lib\site
-packages\transformers\modeling_utils.py", line 2861, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. 
Please use the model as it is, since the model has already been set to the corr
ect devices and casted to the correct `dtype`.
